Our proposed goal was to map the degrees of separation between one Twitch user to another Twitch user. By determining the shortest path/least degrees of separation in combination with the distance between users’ locations (users), we would then be able to view different Twitch niches and circles. Therefore, we’d be able to view similarities in games played and streaming habits between users. These algorithms could potentially be used to recommend users to follow and understand streaming habits of larger Twitch streamers, ultimately enabling greater community and intrapersonal tie detections. We planned to do this through using Dijkstra’s Algorithm, BFS, and Betweenness Centrality. We successfully implemented Dijkastra’s Algorithm, BFS, and Betweenness Centrality, for the most part. We used a Breadth-First search algorithm to traverse our graph of Twitch users and their following. In order to find the shortest number of degrees of separation, represented by the shortest path, between two users, we used Dijkstra’s algorithm which will take in the edges E and vertices V and output the shortest path of V’s and E’s from one user to another. And we determined the most influential and connected vertex through the Betweenness Centrality algorithm. We were able to implement everything we were aiming for; however, it does not run quite as fast or efficiently as we’d be originally hoping for. Additionally, our algorithm does not work if the dataset is too big.
Our graph was composed of private structs and variables for storage. One such private struct is our Node struct, which has variables int twitch_id, pair<double, double> coordinates, and vector<int> edges, which are indices to edge vectors in the graph. Our Edge struct has int to and int from which both represent ids of Nodes. It also has double distance, which is the weighted component of our graph, and the method int getAdj(Node source), which returns the node on the other end of the passed source Node. For storage, we have two vectors, one which is full of Node pointers and the other full of Edge pointers. As for parsing the graph, we decided on parsing the graph into a structure of vector<pair<int, int > >, and vector< pair< pair<int, int>, pair<double, double>>>. We originally had our data in a csv, but changed it to a .txt file to make it easier to parse, which we did through the use of ifstream to read file line by line. 
For our Breadth-First search algorithm, we were successfully able to implement that. Our algorithm returns a vector of ints to store. Our only issue with BFS is that it doesn’t feed Nodes one by one, rather, it traverses the entire subgraph at once. We were also able to successfully implement Dijkstra’s algorithm. Initially, we wanted to use BFS, but BFS traverses through the entire subgraph while Dijkstra stops when the wanted end node is found. With Dijkstra’s, we had an issue where the data passed in was twitch ids, but we needed to to find the index of the twitch id in our Graph DS. To tackle this issue, we decided to store our conversions as a map, which takes O(V) space but saves time significantly. Finally, we were able to successfully implement Betweenness Centrality, in which we calculated the betweenness factor of each node through the path retrieved between two non-overlapping nodes from Dijkstra’s. We return the twitch_id of the node with the greatest betweenness factor, and from there, we can determine that that node is the central node. If there is no central node, we return -1. 
In conclusion, we were essentially able to implement what we set out to do. We can derive the shortest path from user to user and determine the given central user. Essentially, the algorithms we implemented can be used to detect ties amongst users. Possible future implementations would be Page Rank, to provide more relevant recommendations of users to others. Additionally, we could optimize our algorithms. For example, for BFS, we could provide Nodes one by one by using Iterators rather than feeding in the entire path at once. Another possible future improvement is if we could parse the data as a .csv file instead of a .txt file, or make use of the extra data in the datasets to add cool additional functionality to our code. Finally, we could possibly improve our testing infrastructure, since we wrote our tests as we went along, resulting in a good portion of  our test cases being separate entities. A lot of test cases required a small, medium, or large graph to be constructed before checks could be made. In the future, it would be more optimal to design various graphs using functions test cases could call, build test cases around those graphs, and write helper functions to automate our checks.
